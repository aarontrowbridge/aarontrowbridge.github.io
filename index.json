[{"content":"As this planet completes yet another revolution around a pocket of spacetime curvature abounding with scintillating, mingling ions \u0026mdash; and while the sparks seem to be flying in all of the wrong ways here on earth \u0026mdash; I wanted to share some thoughts on a few of the books I read this year. I will in no way do these works justice, but hopefully, I can convey some of the insights, joys, laughs, and tears that they brought me.\n\u0026ldquo;E il naufragar m\u0026rsquo;è dolce in questo mare (and sweet to me is foundering in the sea)\u0026rdquo; \u0026ndash; Giacomo Leopardi, via Italo Calvino\nWhat is Existentialism? \u0026ndash; Simone de Beauvoir This book I read at the beginning of this year; it is composed of two essays: What is existentialism? and Pyrrhus and Cineas.\nThe philosophy under discussion is a response to the diverging threads of self-identity in christianity and marxism: in christianity we have the self, or the soul, as something separate from the world, aching to escape the world; while with marxism the human experience is not disentanglable from the physical world; Pascal referred to man as a \u0026ldquo;thinking reed\u0026rdquo;, engrained, waving in the ebbs and flows of life.\nTo quote Simone, existentialism \u0026ldquo;postulates the value of the individual as the source and reason for being [raison d\u0026rsquo;être]\u0026rdquo;. This philosophy combines individualism with marxist realism and bids us to be in the world and of the world; we can affect the world, just as we are an effect of the world.\nI will leave off with a favorite quote of mine from the book, which is a rallying cry to all of us who have the urge to conquer something, like Pyrrhus, but are unsure of exactly why they should do so:\n\u0026ldquo;Each new act of surpassing [dépassement] gives anew the thing surpassed, and that\u0026rsquo;s why technologies are ways of appropriating the world: the sky belongs to those who know how to fly; the sea belongs to those who know how to swim and navigate. Thus our relationship with the world is not decided from the onset; it is we who decide.\u0026rdquo;\nThe Cyberiad \u0026ndash; Stanislaw Lem It\u0026rsquo;s not possible for me to compare this book to anything else, it is pure brilliance that had me unable to control my laughter throughout. It is comprised of number of \u0026ldquo;sallies\u0026rdquo; where the brilliant constructors Trurl and Klapaucius are called upon for their prodigious talents. Turning now to a random page, to give a taste of this masterpiece, I find words such as \u0026ldquo;drasticodracostochastic control\u0026rdquo; and \u0026ldquo;discontinuous orthodragonality\u0026rdquo;.\nIt\u0026rsquo;s hard for me to describe this without spoiling details, so will not say much more, other than that you (the reader who has made it this far) should read this book. I\u0026rsquo;ll leave you with a bit of poetry that I found (in practice) that can impress attractive bartenders:\nEllipse of bliss, converge, O lips divine!\nThe product of our scalars is defined!\nCyberiad draws nigh, and the skew mind\ncuts capers like a happy haversine.\nI see the eigenvalue in thine eye,\nI hear the tender tensor in thy sigh.\nBernoulli would have been content to die\nHad he but known such $a^2 \\cos 2\\varphi$!\nThis and much more awaits you in The Cyberiad.\nBabel \u0026ndash; R. F. Kuang I am not a big reader of fantasy (I would even admit this is the first book of fantasy I\u0026rsquo;ve read as an adult), but this book, while definitely fantastical in some regards, used the magic silver working aspect as a way to beautifully connect the art (and to some extent science) of language, etymology, and translation with the poignant realities of power, war, and the human urge to stand up to oppression.\nI found myself enamored with the scholarly world of Oxford and the intriguing, inimical practice of translation. I learned to look at language in a new way \u0026mdash; not least because I am currently trying to teach myself french and italian \u0026mdash; while also being reminded of the power that language has to shape the world. The ancient battle between the tongue and the sword comes to a head in this story, and we are left to watch as decisions are made that we may never be capable of fully understanding. In light of recent events in the world, dealing with rhetoric and violence, it is a timely read, if only to grapple with analogous situations in a different context.\nThe Penguin Book of Jewish Short Stories \u0026ndash; edited by Emanuel Litvinoff The caliber of writing in this collection is astounding. In relation to the semi-fictional world of Babel, here we get to see first-hand accounts of people facing truly incomprehensible horrors and coming away from them. Wit, wisdom, and wiley humor are on full display in these stories which stretch from the pogroms of eastern Europe to fortune-making in the mid-twentieth century upstate New York real estate market.\nThis collection includes the stories of\nI. L. Peretz Sholem Aleichem Lamed Shapiro Abraham Reisen S. Y. Agnon Isaac Babel Isaac Bashevis Singer Emmanuel Litvinoff Aharon Appelfeld Dan Jacobson Philip Roth Saul Bellow Cynthia Ozick Amos Oz Bernard Malamud Muriel Spark Millennia of knowledge and experience has been accumulated and put to paper by these authors and I can\u0026rsquo;t recommend this collection enough as a way to see the world through the eyes of a people who have been through so much, giving back all along the way, and have so much yet to give.\nSix Memos for the Next Millennium \u0026ndash; Italo Calvino This collection of lectures \u0026mdash; which includes just five, with the sixth, on consistency, being left unfinished \u0026mdash; highlights the literary genius of Italo Calvino. While these lectures were written in 1985, they anticipated so well the world we live in today, and if anything, his words are more true now than ever.\nLightness Italo begins by describing literature \u0026ldquo;as an existential function\u0026rdquo;, that is, \u0026ldquo;the search for lightness as a reaction to the weight of living\u0026rdquo;. The atomism, which Lucretius was so fond of, is used to illustrate the paradox \u0026mdash; which still holds its validity today \u0026mdash; of the lightness, and inert ephemerality, of the fundamental particles (or really fields) that yield atoms, chemistry, biology, and eventually, us. Via a Kafka story of a flying bucket, we are left, at the end of this lecture, with the fact that \u0026ldquo;the fuller it is, the less it will be able to fly.\u0026rdquo;\n\u0026ldquo;Thus, astride our bucket, we shall face the new millennium, without hoping to find anything more in it than what we ourselves are willing to bring into it.\u0026rdquo;\nQuickness The essence of this attribute can be found by contrasting and comparing the Olympian messenger god Mercury (his metamorphoses, winged flights, and instantaneous expressiveness) with the craftsman god Vulcan (his deliberate, methodical, and painstaking perfectionism). Calvino tells us that \u0026ldquo;Mercury\u0026rsquo;s swiftness and mobility are needed to make Vulcan\u0026rsquo;s endless labors become bearers of meaning.\u0026rdquo; In other words, as a writer today, one needs to be as precise as possible, but also one needs to be able to terminate the process of creation in mercurial flourish, as time is of the essence.\nFestina lente, hurry slowly\nExactitude When dealing with the world via literature, we are faced with multitudinous (maybe even infinite) ways of describing phenomena. Science is of little help \u0026mdash; our knowledge is inherently stratified, expanding, and incomplete. \u0026ldquo;The human mind cannot conceive the infinite, and in fact falls back aghast at the very idea of it, it has to make do with what is indefinite.\u0026rdquo; Utilizing language to achieve maximal clarity, or exactitude, is the challenge thus faced; words connect \u0026ldquo;the visible trace with the invisible thing, the absent thing, the thing that is desired or feared, like a frail emergency bridge flung over an abyss.\u0026rdquo;\nThis is no easy task, none other than Leonardo da Vinci filled his notebooks with iterated sentences, trying to grasp the essence of the thing he was contemplating. We should all learn to love and master this process.\n\u0026ldquo;the real work consists not in its definitive form, but in the series of approximations made to attain it\u0026rdquo;\nVisibility In existing, in creating, at a certain point we all must \u0026ldquo;start putting black on white\u0026rdquo;. To stand out, we must be able to focus our own mind\u0026rsquo;s eye on the elements of our memories, sensations, and experiences that will enable us to realize the \u0026ldquo;world of potentialities\u0026rdquo;, that is our imagination. The problem is one of undecidablilty, that is, \u0026ldquo;the paradox of an infinite whole that contains other infinite wholes\u0026rdquo;.\nIn this year in particular, with the advent of generative AI, we are facing a whole new level to the challenge identified by Calvino with the question:\n\u0026ldquo;Will the power of evoking images that are not there continue to develop in a human race increasingly inundated by a flood of prefabricated images?\u0026rdquo;\nMultiplicity This year seems to me as if humanity is finally coming to bat regarding the state of our knowledge of the world. Calvino in this essay defends the novel that attempts an encyclopedic scope (since the noun encyclopedia literally means an encircling of all knowledge, this is a bit paradoxical); each one comes up short in some way, but the attempt is a noble one. The attempt by an author to grok (to use the timely term) every field of knowledge and bring it into his creation in a way that does it justice is maybe the most honorable of all literary pursuits. In practice, this is infeasible unless one allows himself to be beholden to rules, that will allow him to tame the infinite. I won\u0026rsquo;t philosophize here on how this could apply to our society today, but I will end with a quote of Raymond Queneau, via Calvino:\n\u0026ldquo;Now this sort of inspiration, which consists in blindly obeying every impulse, is in fact slavery. The classical author who wrote his tragedy observing a certain number of known rules is freer than the poet who writes down whatever comes into his head and is slave to other rules of which he knows nothing.\u0026rdquo;\nConsistency noun, conformity in the application of something, typically that which is necessary for the sake of logic, accuracy, or fairness\nit., consistenza; fr., cohérence\nSiddhartha \u0026ndash; Hermann Hesse Listen, to the river, and everything else.\n","permalink":"https://aarontrowbridge.github.io/posts/book-review-2023/","summary":"As this planet completes yet another revolution around a pocket of spacetime curvature abounding with scintillating, mingling ions \u0026mdash; and while the sparks seem to be flying in all of the wrong ways here on earth \u0026mdash; I wanted to share some thoughts on a few of the books I read this year. I will in no way do these works justice, but hopefully, I can convey some of the insights, joys, laughs, and tears that they brought me.","title":"six notable books I read in 2023"},{"content":"In the realm of mathematical finance there are two relatively disjoint worlds: the \u0026ldquo;buy-side\u0026rdquo; and the \u0026ldquo;sell-side\u0026rdquo;. Buyers are analyzing the available financial products (e.g. stocks, bonds, derivatives, etc.) and are trying to predict, and profit off of, the future prices of these assets\u0026mdash;i.e. they are trying to estimate the real-world probability distribution, commonly denoted $P$, of the future stochastic price movements.\nOn the other side, sellers (aka market-makers) are trying to set a fair price for more illiquid assets, like derivatives, using the prices of more liquid assets, like stocks, which are priced by supply and demand. One particular challenge is that investors all have different risk preferences. One way to quantify this risk is with the sharp ratio, which measures risk relative to a numeraire, like a US treasury security, which has a risk-free rate of return $r$. One model for the price of derivative is to use a stochastic discount factor (SDF), also called a pricing kernel, $M_t$. The price $F$ of a derivative at time $t$, which pays out at future time $T$, is then given by\n$$ F_t = \\mathbf{E}_{P}\\left[M_T F_T\\right] = e^{-r(T-t)} \\mathbf{E}_{Q}\\left[ F_T \\right] $$\nWhere in the last step we factored out the risk-free interest and merged the SDF into the probability measure. This is allowed given the measures are equivalent\u0026mdash;written $P \\sim Q$\u0026mdash;and we assume no arbitrage exists; the fundamental theorem of asset pricing encapsulates this.\nPricing derivatives using the risk-neutral measure $Q$ comes with a few advantages over the real world measure $P$. For one, it is often simpler\u0026mdash;requires fewer parameters, and we don\u0026rsquo;t need to worry about SDF\u0026mdash;to calibrate pricing models; two, as we will see, the underlying asset becomes a martingale w.r.t. $Q$; lastly, we don\u0026rsquo;t need to account for investors specific risk preferences.\npreliminaries Let\u0026rsquo;s get some definitions out of the way before deriving the main result below. We will start with the definition of a martingale process, and then define what is required for two probability measures to be called equivalent.\nFor the remainder of the article we will work on the probabilty space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F_t}\\}, P)$, where $\\Omega$ is the sample space of possible paths, $\\mathcal{F}_t$ is the filtration (think available information) of the $\\sigma$-algebra $\\mathcal{F}$ up to time $t$, and $P$ is the probability measure on the sample space.\nNote that a sample path, denoted $\\omega \\in \\Omega$, allows us to think of stochastic variables, given a sample path, as deterministic functions of time: $t \\mapsto X_t(\\omega)$\nmartingales Intuitively a martingale process is one which is not biased, i.e. it does not drift. This can be made rigorous by requiring that on a stochastic process $Y_t$ satisfies (using a generalization of conditional expectation)\n$$ \\mathbf{E}_P[Y_t | \\mathcal{F}_s] = Y_s $$\nfor $s \u0026lt; t$. This says that the conditional expectation of the future value, given the present information, will always be equal to the present value.\nFor Ito processes, it is sufficient that the drift term is zero.\nequivalence of measures A probability measure is not unique on a sample space is not unique; changing measures requires some notion of equivalence. Chiefly, if we want to transform $P \\to Q$, we must have\n$$ P(H) = 0 \\implies Q(H) = 0 $$\nfor all $H \\in \\mathcal{F}_T$, where we fix $t = T$. We then say $Q$ is absolutely continuous w.r.t. $P$ and write $Q \\ll P$, which by the Radon-Nikodym theorem occurs only if\n$$ dQ(\\omega) = Z_T(\\omega)dP(\\omega) $$\nHere our $Z_T(\\omega) = Z_t(\\omega) \u0026gt; 0$ a.s., which implies $P\\ll Q$, allowing us to assert equivalence and write $P \\sim Q$.\nGirsanov\u0026rsquo;s theorem Given an Ito process $X_t$ on the probability space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_t \\}, P)$, described by the stochastic differential equation\n$$ dX_t = \\mu(t, \\omega) dt + \\sigma(t,\\omega)dW_t \\tag{1} $$\nwhere $\\omega \\in \\Omega$ represents a sample path in the sample space of all possible paths, $\\mathcal{F}_t$ is the filtration of the $\\sigma$-algebra $\\mathcal{F}$ up to time $t$ (not actually important for what are we are doing), and most importantly $W_t$ is a wiener process (brownian motion) with respect to the measure $P$.\nLet\u0026rsquo;s now define a new process\n$$ Z_t = \\exp \\left( -\\int_0^t u(s, \\omega) dW_s - {1 \\over 2} \\int_0^t u(s, \\omega)^2 ds \\right), $$\nwhich satisfies the following SDE, where we now make the simplifying assumption that $u(t, \\omega) = u(t)$\u0026mdash;i.e. it is deterministic\u0026mdash;\n$$ dZ_t = -u(t) Z_t dW_t. $$\nNow, if we use the product rule for Ito calculus\n$$ d(X_t Y_t) = Y_t dX_t + X_t dY_t + dX_t dY_t $$\nand if we assume $\\mu(t)$ and $\\sigma(t)$, found in (1), are deterministic, and define u(t) by\n$$ \\mu(t) - \\sigma(t) u(t) = \\alpha(t); $$\nIn general, for an $n$-dimensional Ito process, in terms of an $m$-dimensional Wiener process $W_t$, $\\mu \\in \\mathbb{R}^n$, $\\sigma \\in \\mathbb{R}^{n\\times m}$, and $u \\in \\mathbb{R}^m$. So in higher dimensions invertability of $\\sigma$ becomes an issue, but that will not be discussed here.\nso, with X_t defined in (1), we find that the process $Y_t = Z_t X_t$, where, importantly (for simulation purposes), the noise process $W_t$ is shared, satisfies\n$$ dY_t = d(Z_t X_t) = \\alpha(t) Z_t dt + (Z_t \\sigma(t) - Y_t u(t)) dW_t $$\nWhich means, WLOG, we can make $Y_t$ a martingale by choosing $\\alpha(t) = 0$, eliminating the drift term. In $n = m = 1$ dimensions, we then have $u(t) = {\\mu(t) \\over \\sigma(t)}$.\nthe risk-neutral measure Looking at the above derivation, as well as the definition for equivalence of measures we can define an equivalent martingale measure, or risk-neutral measure, $Q$ on a derivative paying out at time $T$ modeled by an Ito process, by\n$$ dQ = \\exp \\left( -\\int_0^T u(s,\\omega)dW_s - {1 \\over 2}\\int_0^T u^2(s,\\omega)ds \\right)dP, $$\nsince, accounting for the discount rate,\n$$ \\begin{align*} \\mathbf{E}_P\\left[ Z_T e^{-rT}X_T\\right] \u0026amp;= e^{-rT}\\mathbf{E}_P\\left[ Z_T X_T\\right] \\\\\\ \u0026amp;= e^{-rT}\\mathbf{E}_Q\\left[ X_T\\right] \\end{align*} $$\nWe can also check that $Q$ is indeed a probability measure:\n$$ \\mathbf{E}_Q[1] = \\mathbf{E}_P[Z_t] = \\mathbf{E}_P[Z_0] = \\mathbf{E}_P[1] = 1 $$\nusing the fact that $Z_t$ is a martingale.\nfinal thoughts and a visualization This machinery gives us a way to calculate risk neutral expectations: calibrate $Z_t$ by fitting the parameters of a model for the underlying asset and use that measure to calculate expectations of more complicated derivatives. In practice there are fancier tricks, that I may eventually deliberate on.\nIntuitively, the risk-neutral measure doesn\u0026rsquo;t change the paths of the process, it just weights the paths s.t. the expectation over all paths is equal to the starting position. For a simple model this looks like this:\nThis plot was generated with a $1$-dimensional Ito process with $\\mu = 1.5$ and $\\sigma = 1$. Code can be found here.\n","permalink":"https://aarontrowbridge.github.io/posts/equivalent-martingale-measures/","summary":"In the realm of mathematical finance there are two relatively disjoint worlds: the \u0026ldquo;buy-side\u0026rdquo; and the \u0026ldquo;sell-side\u0026rdquo;. Buyers are analyzing the available financial products (e.g. stocks, bonds, derivatives, etc.) and are trying to predict, and profit off of, the future prices of these assets\u0026mdash;i.e. they are trying to estimate the real-world probability distribution, commonly denoted $P$, of the future stochastic price movements.\nOn the other side, sellers (aka market-makers) are trying to set a fair price for more illiquid assets, like derivatives, using the prices of more liquid assets, like stocks, which are priced by supply and demand.","title":"risk neutrality: the equivalent martingale measure"},{"content":"In 2014, Ian Goodfellow, then at OpenAI, published his seminal paper, titled \u0026ldquo;Generative Adversarial Networks\u0026rdquo;1, detailing how competition between generator and discriminator functions, approximated by neural networks, can train the generator to produce realistic images.\nIn this article I will be discussing the theory behind this idea, my own implementation in Julia (mirroring the network structure in Goodfellow\u0026rsquo;s original paper), and show some of the images I was able to get out.\ntheory To begin with let\u0026rsquo;s model our generator as a parameterized function $G(z; \\theta)$ that outputs images that are the same size as the data; this function is over a lower dimensional latent space, which we assign a prior distribution $p_z(z)$.\nWe will also model the discriminator as a parametrized function $D(x; \\omega)$ which takes in an image $x$ and outputs a single scalar interpreted as the probability of $x$ being in the training data set.\nIn practice, $D$ and $G$ are usually both implemented as neural networks, either fully connected multi-layer perceptrons (MLPs) or convolutional and \u0026ldquo;deconvolutional\u0026rdquo;.\nDeconvolutional layers, also called convolutional transpose layers, \u0026ldquo;upsample\u0026rdquo; from a lower dimensional space to a higher dimensional space, and while the term is sometimes used, it implies an inverse to a convolutional layer, which it is not. The alternative name (convolutional transpose) is more apt and more frequently used lately \u0026ndash; at least Flux.jl implements a ConvTranspose layer.\nthe minimax problem At a high level, in training these networks, we want $D$ to accurately tell a real image from a fake image and we want $G$ to trick $D$ into believing the generated images are real. This results in the minimax optimization problem:\n$$ \\begin{equation} \\min_G \\max_D \\ \\mathbb{E}_{x \\sim p_{data}(x)}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p_z(z)} \\left[ \\log\\left(1 - D(G(z)) \\right)\\right] \\end{equation} $$\nHere, $1 - D(G(z))$ is the probability that a generated image is fake, which we want to minimize w.r.t. $G$, while simultaneously maximizing $D$\u0026rsquo;s ability to tell real from fake.\nUsing the value function in (1), denoted $V(G, D)$, we can iteratively update $G$ and $D$, in turn, using minibatch stochastic gradient descent; see below or Algorithm 1 in [Goodfellow et al.] for details.\nglobal optimality Ultimately, we want the distribution of generated images to match the data distribution, i.e., $p_g = p_{\\text{data}}$.\nFor any fixed (suboptimal) generator $G$ we want to find $D^*_G(x)$ which maximizes $V$:\n$$ \\begin{align} V(G, D) \u0026amp;= \\int_x p_{\\text{data}}(x) \\log(D(x)) \\ \\mathrm{d}x + \\int_z p_{z}(z) \\log(1 - D(G(z))) \\ \\mathrm{d}z \\nonumber \\\\\\ \u0026amp;= \\int_x p_{\\text{data}}(x) \\log(D(x)) + p_{g}(x) \\log(1 - D(x)) \\ \\mathrm{d}x \\end{align} $$\nThe integrand of (2), which has the form $y \\to a \\log y + b \\log(1 - y)$ with $(a, b) \\in \\mathbb{R}^2 \\backslash \\{0,0\\}$ with a maximum in $[0,1]$ at $a \\over a+b$. So we then have\n$$ \\begin{equation} D^*_G(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} \\end{equation} $$\nNow, looking back at (1) we can see that we need to minimize\n$$ \\begin{align} C(G) \u0026amp;= \\max_D V(G, D) = V(G, D^*_G) \\nonumber \\\\\\ \u0026amp;= \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D^*_G(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D^*_G(G(z))] \\nonumber \\\\\\ \u0026amp;= \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D^*_G(x)] + \\mathbb{E}_{x \\sim p_g}[\\log(1 - D^*_G(x))] \\nonumber \\\\\\ \u0026amp;= \\mathbb{E}_{x \\sim p_{\\text{data}}}\\left[\\log \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} \\right] + \\mathbb{E}_{x \\sim p_g}\\left[\\log \\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)} \\right] \\end{align} $$\nWith equation (3) we can clearly see that $D^*_G(x) = {1 \\over 2}$ when $p_g = p_{\\text{data}}$, which implies that at its absolute minimum\n$$ C(G^*) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[-\\log 2] + \\mathbb{E}_{x \\sim p_g}[-\\log 2] = -\\log 4. $$\nWe now note that the Kullback-Leibler divergence is defined as\n$$ \\begin{align*} D_{KL}(p \\ \\Vert \\ q) \u0026amp;\\equiv \\int_x p(x) \\log {p(x) \\over q(x)} \\ \\mathrm{d}x \\\\\\ \u0026amp;= \\mathbb{E}_{x \\sim p(x)} \\left[ \\log {p(x) \\over q(x)} \\right] \\end{align*} $$\nand the Jenson-Shannon divergence, which is symmetric, strictly positive, and $0$ iff the distributions are equal, is defined as\n$$ \\begin{align*} D_{JS}(p \\ \\Vert \\ q) \u0026amp;\\equiv {1 \\over 2}D_{KL}\\left(p \\ \\Bigg\\Vert \\ {p + q \\over 2} \\right) + {1 \\over 2}D_{KL}\\left(q \\ \\Bigg\\Vert \\ {p + q \\over 2} \\right) \\\\\\ \u0026amp;= {1 \\over 2} \\mathbb{E}_{x \\sim p(x)} \\left[ \\log {2 \\ p(x) \\over p(x) + q(x)} \\right] + {1 \\over 2} \\mathbb{E}_{x \\sim q(x)} \\left[ \\log {2 \\ q(x) \\over p(x) + q(x)} \\right] \\\\\\ \u0026amp;= {1 \\over 2} \\left( \\mathbb{E}_{x \\sim p(x)} \\left[ \\log {p(x) \\over p(x) + q(x)} \\right] + \\mathbb{E}_{x \\sim q(x)} \\left[ \\log {q(x) \\over p(x) + q(x)} \\right] + \\log 4 \\right) \\end{align*} $$\nThus, using (4), the cost function for the generator $G$, for optimal $D^*_G$, can be written as\n$$ \\begin{equation} C(G) = -\\log 4 + 2 \\cdot D_{JS}(p_{\\text{data}} \\ \\Vert \\ p_g). \\end{equation} $$\noptimization The above analysis shows that our loss for $G$ is well defined when $D$ is close to optimality. In order to get the discriminator to be as close to optimal as possible, we can train it in an inner loop with $k$ updates for ever one update to the generator. This procedure schematically goes as follows:\nAlgorithm minibatch SGD training of generative adversarial nets\nfor $n$ iterations:\nfor $k$ inner loops:\nsample $m$ noise samples $z_i \\sim p_g(z)$ sample $m$ data samples $x_i \\sim p_{\\text{data}}(x)$ update $D$ to ascend via SGD with $$ \\nabla_\\omega \\ {1 \\over m} \\sum_{i=1}^m \\log D\\left(x_i; \\ \\omega\\right) + \\log \\big( 1 - D(G(z_i; \\ \\theta); \\ \\omega)\\big) $$ sample $m$ noise samples $z_i \\sim p_g(z)$ update $G$ to descend via SGD with $$ \\nabla_\\theta \\ {1 \\over m} \\sum_{i=1}^m \\log \\big( 1 - D(G(z_i; \\ \\theta); \\ \\omega)\\big) $$ The plots below show what the training loss dynamics looks like for different values of $k$:\nWe can see that the generator $G$\u0026rsquo;s loss decays more steeply when $k$ is larger, which is good, but comes at a cost: the computational cost increases steeply. This trade off should be considered when training.\nexperiments An implementation of the \u0026ldquo;simple\u0026rdquo; GAN model described here (plus some more advanced models that aren\u0026rsquo;t yet implemented at the time of writing this) can be found on github at FluxGAN.jl. The package runs on the gpu (if available) and can be easily installed and used.\ntraining Visualizing the training of a GAN can be tricky; I found it helpful to make the following gif which shows the generator\u0026rsquo;s output for specific points in the latent space during the training process.\nThe following are randomly sampled outputs from generators trained with different architectures on different data sets, all with $k=1$.\nMNIST fully connected network (code)\nCIFAR-10 convolutional network (code)\nfully connected network (code)\nGoodfellow et al. (2014) \u0026ldquo;Generative Adversarial Networks\u0026rdquo; arXiv:1406.2661\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://aarontrowbridge.github.io/posts/generative-adversarial-nets/","summary":"In 2014, Ian Goodfellow, then at OpenAI, published his seminal paper, titled \u0026ldquo;Generative Adversarial Networks\u0026rdquo;1, detailing how competition between generator and discriminator functions, approximated by neural networks, can train the generator to produce realistic images.\nIn this article I will be discussing the theory behind this idea, my own implementation in Julia (mirroring the network structure in Goodfellow\u0026rsquo;s original paper), and show some of the images I was able to get out.","title":"generative adversarial networks"},{"content":"Physics quantum gravity gauge theory and quantum gravity (hand-written notes)\nan overview of a quantum gravity \u0026ndash; focusing on the graviton propagator and developing the gauge theoretic formulation with the Chern-Simons action these notes were used to give a blackboard lecture as a final project for graduate quantum field theory I at Syracuse University an accelerated metropolis algorithm for lattice quantum gravity simulations (slides)\nthese beamer slides detail work I did on a novel version of the metropolis algorithm part of a talk given at an undergraduate research festival at Syracuse University ","permalink":"https://aarontrowbridge.github.io/notes/","summary":"Physics quantum gravity gauge theory and quantum gravity (hand-written notes)\nan overview of a quantum gravity \u0026ndash; focusing on the graviton propagator and developing the gauge theoretic formulation with the Chern-Simons action these notes were used to give a blackboard lecture as a final project for graduate quantum field theory I at Syracuse University an accelerated metropolis algorithm for lattice quantum gravity simulations (slides)\nthese beamer slides detail work I did on a novel version of the metropolis algorithm part of a talk given at an undergraduate research festival at Syracuse University ","title":"Notes"},{"content":"So you want inline interactive plots in your blog posts? But you don\u0026rsquo;t want to learn javascript and there wasn\u0026rsquo;t an obvious way to embed matplotlib?\nIf your comfortable with (or willing to learn) Julia, there is a simple and elegant solution! Makie.jl is a julia plotting package with a lot of functionality, including a backend system consisting of WGLMakie and JSServe which provides a way to embed makie plots\u0026mdash;including 3d visualizations\u0026mdash;right into html and inserted into markdown jekyll blog posts.\nDISCLAIMER: plots may not appear on some browsers, specifically safari, as WebGL is required and might not be supported. Also, loading the page might take a couple seconds.\na first example A simple script to display a static 3d visualization\u0026mdash;a contour plot of a random volume\u0026mdash;looks like this:\nusing WGLMakie, JSServe WGLMakie.activate!() n = 10 volume = rand(n, n, n) fig, ax, plotobj = contour(volume, figure=(resolution=(700, 700),)) output_file = \u0026#34;static/plot_html/interactive_plotting/contour.html\u0026#34; # this is the essential component open(output_file, \u0026#34;w\u0026#34;) do io println(io, \u0026#34;\u0026lt;center\u0026gt;\u0026#34;) show(io, MIME\u0026#34;text/html\u0026#34;(), Page(exportable=true, offline=true)) show(io, MIME\u0026#34;text/html\u0026#34;(), fig) # we insert the figure here println(io, \u0026#34;\u0026lt;/center\u0026gt;\u0026#34;) end To include the plot in a jekyll blog post just use liquid\u0026mdash;i.e.\n{{\u0026lt; include-html \u0026#34;static/plot_html/interactive_plotting/contour.html\u0026#34; \u0026gt;}} which outputs:\nUsing a mouse or touchpad, the figure can be rotated and zoomed. Pretty cool eh?\nclick and drag to rotate mouse wheel or 2 fingers on track pad to zoom This script works by converting the makie fig into an html blob that is written into output_file\nmore interactivity Now lets try something with a little more interactivity; sliders are a good place to start.\nUsing makie we can write:\nusing WGLMakie, JSServe WGLMakie.activate!() # this is optional and just changes the color theme set_theme!(theme_dark()) # radial sinc function with scale parameter \u0026#34;a\u0026#34; radial_sinc(x, y, a) = sinc(a * hypot(x, y)) # domain of surface xs = LinRange(-5, 5, 150) ys = LinRange(-5, 5, 150) # creating the javascript app app = App() do session::Session # create the slider scale_slider = Slider(1:3) # map slider values to surface states states = map(scale_slider) do a return [radial_sinc(x, y, a) for x in xs, y in ys] end # create the figure fig, = surface(xs, ys, states) # show the slider value above the slider # using \u0026#34;\\\\(a = \\\\)\u0026#34; to embed inline latex in markdown scale_value = DOM.div(\u0026#34;\\\\(a = \\\\)\u0026#34;, scale_slider.value) # record a state map for the app and display fig above value above slider return JSServe.record_states( session, DOM.div(fig, scale_value, scale_slider) ) end output_file = \u0026#34;static/plot_html/interactive_plotting/sinc_surface.html\u0026#34; open(output_file, \u0026#34;w\u0026#34;) do io println(io, \u0026#34;\u0026lt;center\u0026gt;\u0026#34;) show(io, MIME\u0026#34;text/html\u0026#34;(), Page(exportable=true, offline=true)) show(io, MIME\u0026#34;text/html\u0026#34;(), app) println(io, \u0026#34;\u0026lt;/center\u0026gt;\u0026#34;) end which outputs:\n\\\u0026#40;a \u0026#61; \\\u0026#41; 1 the future Hopefully this demonstrates how julia can be used to do some pretty cool things in the browser.\nMore sliders can be added as long as they are independent of each other. In the future I will try to figure out dynamics (e.g. moving particles, fluctuating membranes), which I think would be pretty cool. :)\n","permalink":"https://aarontrowbridge.github.io/posts/interactive-julia-plotting/","summary":"So you want inline interactive plots in your blog posts? But you don\u0026rsquo;t want to learn javascript and there wasn\u0026rsquo;t an obvious way to embed matplotlib?\nIf your comfortable with (or willing to learn) Julia, there is a simple and elegant solution! Makie.jl is a julia plotting package with a lot of functionality, including a backend system consisting of WGLMakie and JSServe which provides a way to embed makie plots\u0026mdash;including 3d visualizations\u0026mdash;right into html and inserted into markdown jekyll blog posts.","title":"using Julia to do web based interactive plotting"},{"content":"Courses and textbooks, at least in undergrad, often gloss over the details of the Legendre transformation, which converts convex functions of one variable into another convex function of the \u0026ldquo;conjugate\u0026rdquo; variable. Used ubiquitously in physics, from thermodynamics to quantum field theory, this mathematical method plays a central role in connecting some of the most fundamental concepts, and yet, at least to me for a while, was a mysterious black-box procedure. I am now going to attempt to illuminate this technique.\nLet\u0026rsquo;s begin with the technical definition:\nDefinition Given a convex function $f: I \\subset \\mathbb{R} \\to \\mathbb{R}$ of a variable $x$, the Legendre transform, or convex conjugate, $f^* : I^* \\to \\mathbb{R}$ in terms of the conjugate variable $x^*$ is given by\n$$ \\begin{equation} f^*(x^*) = \\sup_{x \\in I} \\big( x^* x - f(x)\\big), \\ \\ x^* \\in I^* \\end{equation} $$\nwhere\n$$ I^* = \\left\\{ x^* \\in \\mathbb{R} : \\sup_{x \\in I} \\big( x^* x - f(x)\\big) \u0026lt; \\infty \\right\\} $$\nThis definition generalizes to convex functions of higher dimensions by replacing $x^{*}x$ with $\\langle x^*, x \\rangle$, the appropriate inner product.\nBelow is an interactive plot (made with Julia) demonstrating how $f^*(p)$ depends on the maximum of the transformed function $px-f(x)$ for a given function $f(x) = \\frac{1}{2}ax^2 + c$, where here $a = 1$ and $c = 4$. The derivation of these functions will be made clear below, but this might give some intuition into the transformation.\n\\\u0026#40;p \u0026#61; \\\u0026#41; -6.0 Notice that the maximum of the orange curve always lies on the green curve $f^*$.\na more explicit definition For a fixed $x^*$ we can find $f^*(x^*)$ by finding $\\bar x$, s.t. the expression in (1) is maximized, via the standard method from calculus, and the fact that a linear function minus a convex function is a concave function.\n$$ \\begin{align} 0 \u0026amp;= \\frac{d}{dx} \\big( x^* x - f(x) \\big)\\bigg\\rvert_{x=\\bar x} \\nonumber \\\\\\ \u0026amp;= x^* - f\u0026rsquo;(\\bar x) \\nonumber \\\\\\ \\nonumber \\\\\\ \\implies \\bar x \u0026amp;= \\big(f\u0026rsquo;\\big)^{-1}(x^*) \\\\\\ \\nonumber \\end{align} $$\nKeeping in mind that $\\bar x$ depends on $x^*$, we can now write\n$$ \\begin{equation} \\boxed{ f^*(x^{*}) = x^{*} \\bar x - f(\\bar x) } \\end{equation} $$\nwhich is our first explicit definition of the Legendre transform of $f$. Then, taking the derivative, we find that\n$$ \\begin{align*} \\big(f^*(x^{*})\\big)\u0026rsquo; \u0026amp;= \\bar x + x^{*} \\frac{d \\bar x}{dx^{*}} - f\u0026rsquo;(\\bar x) \\frac{d \\bar x}{dx^{*}} \\\\\\ \u0026amp;= \\bar x. \\\\\\ \\end{align*} $$\nThis is true since (2) implies $f\u0026rsquo;(\\bar x) = x^*$, which then implies\n$$ \\begin{equation} \\boxed{ \\big( f^* \\big)\u0026rsquo; = \\big( f\u0026rsquo; \\big)^{-1}. } \\end{equation} $$\nThus (4) is an equivalent way to specify $f^*$ up to an additive constant by integrating both sides of the expression with respect to $x^*$.\nmechanics Considering a mechanical system, we can specify the dynamics of the system by considering the Lagrangian functional for a given path $q(t)$, which, for all intents and purposes can be written as:\n$$ \\mathcal{L}[q(t)] = \\mathcal{L}(q, \\dot q; t) \\equiv \\frac{1}{2}m \\dot q^2 - V(q) $$\nThis defines a function on configuration space (technically the tangent bundle $T\\mathcal{M}$ of a manifold $\\mathcal{M}$), with coordinates $(q, \\dot q)$. Given starting and ending coordinates, the physical solution is given by extremizing the action functional $ S[q(t)] = \\int dt \\ \\mathcal{L}[q]$, resulting in the Euler-Lagrange equation:\n$$ \\frac{d}{dt}\\frac{\\partial \\mathcal{L}}{\\partial \\dot q} =\\frac{\\partial \\mathcal{L}}{\\partial q} $$\nan aside on conjugates Before proceeding, I must comment that there seems to exist inconsistent definitions of conjugate variables or at least inconsistent usage of the term. For example, the classical definition of a conjugate variable in mechanics is the result of differentiating the action with respect to the original variable\u0026ndash;e.g. schematically for $q$\n$$ \\begin{align*} \\frac{\\partial S}{\\partial q} \u0026amp;= \\int dt \\frac{\\partial \\mathcal{L}}{\\partial q} \\\\\\ \u0026amp;= \\int dt \\frac{d}{dt} \\frac{\\partial \\mathcal{L}}{\\partial \\dot q} \\\\\\ \u0026amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\dot q} \\\\\\ \u0026amp;\\equiv p \\\\\\ \\end{align*} $$\nSo $p$ is conjugate to $q$, but the literature (wikipedia) seems to say that $p$ is conjugate to $\\dot q$ in the context of Legendre transforms. I am looking out for clarification regarding this point.\nFor fixed $q$, we can view $\\mathcal{L}$ as a convex function of $\\dot q$, so\n$$ \\mathcal{L}\u0026rsquo;(\\dot q) = m \\dot q \\implies \\big(\\mathcal{L}\u0026rsquo;\\big)^{-1}(p) = \\frac{p}{m}. $$\nWe can now find the Legendre transform of $\\mathcal{L}$ taking $\\dot q \\to p$, using (3):\n$$ \\begin{align*} \\mathcal{L}^*(q, p) \u0026amp;= \\big( p \\dot q - \\mathcal{L}(q, \\dot q) \\big)\\bigg\\rvert_{\\dot q = \\big(\\mathcal{L}\u0026rsquo;\\big)^{-1}(p)} \\\\\\ \u0026amp;= p \\left( \\frac{p}{m} \\right) - \\mathcal{L}\\left( q, \\frac{p}{m} \\right) \\\\\\ \\\\\\ \\implies \\mathcal{H}(q, p) \u0026amp;\\equiv \\frac{p^2}{2m} + V(q) = \\mathcal{L}^*(q, p) \\ \\end{align*} $$\nHere, $\\mathcal{H}$ is the Hamiltonian - a function of the system\u0026rsquo;s phase space (technically the cotangent bundle $T^*\\mathcal{M}$)\nFor completeness\u0026rsquo; sake, let\u0026rsquo;s see how this works with the alternative definition (4). With $q$ fixed\n$$ \\begin{align*} \\mathcal{H}(q,p) \u0026amp;= \\int dp \\ \\left(\\frac{\\partial \\mathcal{L}}{\\partial \\dot q}\\right)^{-1} \\\\\\ \u0026amp;= \\int dp \\ \\frac{p}{m} \\\\\\ \u0026amp;= \\frac{p^2}{2m} + C \\end{align*} $$\nwhich matches (4) when we recognize the constant of integration $C = V(q)$.\n","permalink":"https://aarontrowbridge.github.io/posts/legendre-transformations/","summary":"Courses and textbooks, at least in undergrad, often gloss over the details of the Legendre transformation, which converts convex functions of one variable into another convex function of the \u0026ldquo;conjugate\u0026rdquo; variable. Used ubiquitously in physics, from thermodynamics to quantum field theory, this mathematical method plays a central role in connecting some of the most fundamental concepts, and yet, at least to me for a while, was a mysterious black-box procedure. I am now going to attempt to illuminate this technique.","title":"Legendre transformations"},{"content":"We saw in a previous post that for a non-interacting theory (i.e. $V(\\varphi) = 0$) that the generating functional can be written as\n$$ Z[J] = e^{\\frac{1}{2} J \\cdot K^{-1} \\cdot J}. $$\nWe hinted that it is not always the case that $K$ can be naively inverted. The issue arises when we consider the Maxwell action for a $U(1)$ gauge potential $A_\\mu$:\n$$ S(A) = \\int d^4 x \\left[ \\frac{1}{2} A_\\mu \\left( \\partial^2 g^{\\mu \\nu} - \\partial^\\mu \\partial^\\nu \\right) A_\\nu + A_\\mu J^\\mu \\right]. $$\nHere the operator we are looking to invert is\n$$ \\begin{equation} Q^{\\mu \\nu} \\equiv \\partial^2 g^{\\mu \\nu} - \\partial^\\mu \\partial^\\nu \\end{equation} $$\nwhich has zero eigenvalues for vectors of the form $\\partial_\\mu \\theta(x)$:\n$$ Q^{\\mu \\nu} \\partial_\\mu \\theta(x) = \\left(\\partial^2 g^{\\mu \\nu} \\partial_\\nu - \\partial^\\nu \\partial_\\nu \\partial^\\mu \\right) \\theta(x) = 0 $$\nSo, $Q^{\\mu \\nu}$ is not invertible and the issue is gauge redundancy - we need to fix a gauge.\nthe Faddeev-Popov procedure An interpretation of the issue is that the redundancy in the gauge field, where acting with $g \\in U(1)$, takes $A_\\mu \\to A_\\mu - \\partial_\\mu \\theta \\equiv A_g$; which is not a physical symmetry of the system, but still gets naively integrated over in the generating functional:\n$$ \\begin{align*} Z \u0026amp;= \\int \\mathcal{D}A \\ e^{iS(A)} \\\\\\ \u0026amp;= \\int \\mathcal{D}A \\int \\mathcal{D}g \\ e^{iS(A_g)} \\\\\\ \\end{align*} $$\nWhich diverges.\nThe naive path integral effectively over counts and blows up. What we need to do is insert a constraint on the gauge (e.g. $f(A_g) = 0$), which can be done by inserting\n$$ \\begin{equation} 1 = \\int \\mathcal{D}g \\ \\delta\\left( f(A_g) \\right) \\det \\left( \\frac{\\partial f(A_g)}{\\partial{g}} \\right) = \\Delta(A)\\int \\mathcal{D}g \\ \\delta\\left( f(A_g) \\right), \\end{equation} $$\nwhere $\\Delta(A)$ is the Faddeev-Popov determinant which is gauge invariant. We then get\n$$ \\begin{aligned} Z \u0026amp;= \\int \\mathcal{D}A \\ e^{iS(A)} \\\\\\ \u0026amp;= \\int \\mathcal{D}A \\ e^{iS(A)} \\ \\Delta(A) \\int \\mathcal{D}g \\ \\delta\\left( f(A_g) \\right) \\\\\\ \u0026amp;= \\int \\mathcal{D}g \\int \\mathcal{D}A \\ e^{iS(A)} \\ \\Delta(A) \\ \\delta\\left( f(A_g) \\right) \\\\\\ \u0026amp;= \\left( \\int \\mathcal{D}g \\right) \\int \\mathcal{D}A \\ e^{iS(A)} \\ \\Delta(A) \\ \\delta\\left( f(A) \\right) \\\\\\ \\end{aligned} $$\nWhere we arrived at the final result by noting that the path integral measure $\\mathcal{A}$ is gauge invariant and shifting $A \\to A_{g^{-1}}$ removes the gauge dependence from $\\delta(f(A_g))$. The factor outside, $\\left( \\int \\mathcal{D}g \\right)$ is an infinite constant that we can drop, as it has no physical effect.\nTo see how this procedure works with QED check out Zee\u0026rsquo;s wonderful book Quantum Field Theory in a Nushell, which was my primary reference for this post.\nghosts in non-abelian gauge theories We will now turn our attention towards quantum Yang-Mills theory, where the gauge group being considered is non-abelian, for all intents and purposes is $SU(N)$. Here things get very spooky.\nIn a pure non-abelian gauge theory, each component of the gauge potential $A_\\mu$ transforms under the adjoint representation of the gauge group, in such a way that the action remains gauge invariant\u0026ndash;i.e.\n$$ A_\\mu(x) \\to U(x)A_\\mu(x)U^\\dagger(x) + iU(x)\\partial_\\mu U^\\dagger(x) $$\nOne can note that if $U$ does not vary throughout space, $A$ really does transform in the adjoint representation; the extra term comes from the requirement for gauge invariance in a theory where the gauge group can act differently at each spacetime coordinate.\nNow, given a set of generators for the gauge group, ${T^a}$, really a Lie algebra, we can see that under an infinitesimal gauge transformation ($U(x) \\simeq 1 + i\\theta^a T^a$), and noting we can decompose $A_\\mu = A^a_\\mu T^a $, we see that\n$$ A^a_\\mu \\to A^a_\\mu - f^{abc}\\theta^b A^c_\\mu + \\partial_\\mu \\theta^a. $$\nWhere the structure constants $f^{abc}$ are defined by $[T^a, T^b] = f^{abc}T^c$.\nLet\u0026rsquo;s now choose a gauge fixing condition to be $f(A) = \\partial^\\mu A_\\mu - \\sigma$, with $\\sigma$ arbitrary. By (2) we then have\n$$ \\begin{align*} \\Delta(A) \u0026amp;= \\left\\{\\int \\mathcal{D}\\theta \\ \\delta\\left[ f(A) \\right] \\right\\}^{-1} \\\\\\ \u0026amp;= \\left\\{\\int \\mathcal{D}\\theta \\ \\delta\\left[ \\partial^\\mu A^a_\\mu - \\sigma^a \\right] \\right\\}^{-1} \\\\\\ \u0026amp;= \\left\\{\\int \\mathcal{D}\\theta \\ \\delta\\left[ \\partial^\\mu A^a_\\mu - \\sigma^a - \\partial^\\mu \\left( f^{abc}\\theta^b A^c_\\mu + \\partial_\\mu \\theta^a \\right)\\right] \\right\\}^{-1} \\\\\\ ``\u0026amp;= \\text{\u0026rsquo;\u0026rsquo;} \\ \\left\\{\\int \\mathcal{D}\\theta \\ \\delta\\left[ \\partial^\\mu \\left( f^{abc}\\theta^b A^c_\\mu + \\partial_\\mu \\theta^a \\right)\\right] \\right\\}^{-1} \\\\\\ \\end{align*} $$\nWhere there are quotes around the equality of the last line because we have yet to muliply by the gauge constraint $\\delta[f(A)]$.\nNow lets formally define an operator $K^{ab}(x,y)$ by\n$$ \\partial^\\mu \\left( f^{abc}\\theta^b A^c_\\mu + \\partial_\\mu \\theta^a \\right) = \\int d^4y \\ K^{ab}(x, y) \\theta^b(y). $$\nthat is\n$$ K^{ab}(x,y) \\equiv \\partial^\\mu \\left( f^{abc} A^c_\\mu - \\partial_\\mu \\delta^{ab} \\right) \\delta^{(4)}(x - y) $$\nA generalization of the delta function identity, $\\int d\\theta \\ \\delta(K\\theta) = 1/K$, to a vector $\\theta$ and matrix $K$, gives us $\\int d\\theta \\ \\delta(K\\theta) = 1 / \\det K$. We also have that anti-commuting Grassmann variables allow us to write $\\det K = \\int d\\eta d\\eta^\\dagger \\exp\\left[-\\eta^\\dagger \\cdot K \\cdot \\eta\\right]$. Pulling all of this together we see that\n$$ \\Delta(A) = \\det K^{ab}(x,y) = \\int \\mathcal{D} c \\mathcal{D} c^\\dagger e^{i S_{\\text{ghost}}(c^\\dagger, c)} $$\nwhere\n$$ \\begin{aligned} S_{\\text{ghost}}(c^\\dagger, c) \u0026amp;= \\int d^4x d^4y \\ c_a^\\dagger(x)K^{ab}(x, y)c_b(y) \\\\\\ \u0026amp;= \\int d^4x \\ c_a^\\dagger(x) \\left[ \\partial^\\mu f^{abc} A^c_\\mu(x) c_b(x) - \\partial^2 c_b(x) \\right] \\\\\\ \u0026amp;= \\int d^4x \\ \\left[ \\partial^\\mu c_a^\\dagger(x) \\partial_\\mu c_b(x) - \\partial^\\mu c^\\dagger_a f^{abc} A^c_\\mu(x) c_b(x) \\right] \\\\\\ \u0026amp;= \\int d^4x \\ \\partial^\\mu c_a^\\dagger(x) \\left[ \\partial_\\mu c_b(x) - f^{abc} A^c_\\mu(x) c_b(x) \\right] \\\\\\ \u0026amp;= \\int d^4x \\ \\partial^\\mu c_a^\\dagger(x) D_\\mu c_a(x) \\\\\\ \\end{aligned} $$\nHere $D_\\mu$ is the covariant derivative of the adjoint representation; $c_a, c_a^\\dagger,$ and $A_\\mu^a$ all transform in this way.\n$c_a$ and $c_a^\\dagger$ are ghost fields and seemingly violate the spin-statistics connection, because they are grassmann anti-commuting variables, but they are not physical, just a convenient representation of $\\Delta(A)$.\nZooming out, we now have\n$$ Z = \\int \\mathcal{D}A\\mathcal{D}c\\mathcal{D}c^\\dagger \\ e^{iS(A) + iS_{\\text{ghost}}(c^\\dagger, c)} \\ \\delta(\\partial A - \\sigma) $$\nwhich after integrating over $\\sigma$ with a Gaussian weight, $e^{ -(i/2\\xi) \\int d^4x \\sigma^a(x)^2}$, we arrive at a final gauge fixed (with gauge parameter $\\xi$) generating functional for a pure non-abelian Yang-Mills theory:\n$$ \\boxed{ Z(J) = \\int \\mathcal{D}A\\mathcal{D}c\\mathcal{D}c^\\dagger \\ \\exp \\left\\{iS(A) + iS_{\\text{ghost}}(c^\\dagger, c) - \\frac{i}{2\\xi} \\int d^4x (\\partial^\\mu A_\\mu)^2 + \\int d^4x J^\\mu A_\\mu\\right\\} } $$\n","permalink":"https://aarontrowbridge.github.io/posts/ghosts-gauges-and-generating-functionals/","summary":"We saw in a previous post that for a non-interacting theory (i.e. $V(\\varphi) = 0$) that the generating functional can be written as\n$$ Z[J] = e^{\\frac{1}{2} J \\cdot K^{-1} \\cdot J}. $$\nWe hinted that it is not always the case that $K$ can be naively inverted. The issue arises when we consider the Maxwell action for a $U(1)$ gauge potential $A_\\mu$:\n$$ S(A) = \\int d^4 x \\left[ \\frac{1}{2} A_\\mu \\left( \\partial^2 g^{\\mu \\nu} - \\partial^\\mu \\partial^\\nu \\right) A_\\nu + A_\\mu J^\\mu \\right].","title":"ghosts, gauges, and generating functionals"},{"content":"Quantum field theory is the study of various types of fields, the interactions between these fields, and the correlation functions describing the dynamics of the entire system. Fields are really just functions that label each spacetime coordinate with some type of mathematical object. We can talk about spinor fields, scalar fields, and even gauge fields, which can be described by vectors or tensors that transform in certain ways under an associated gauge group.\nTo describe the properties of fields (which we can bundle up into one big vector $\\varphi$) and their interactions, QFT takes advantage of the associated action functional of the fields,\n$$ S[\\varphi] = \\int d^4x \\ \\mathcal{L}[\\varphi] $$\nwhere $\\mathcal{L}[\\varphi]$ is the Lagrangian density of the system, which specifies a \u0026ldquo;quantum field theory\u0026rdquo;, with its own set of parameters and interactions. Classical field theory can be summarized as finding $\\varphi$ s.t. $S[\\varphi]$ is at a minimum (more accurately a critical point), which is called the principal of least action.\nYou may then be wondering, like I was, what makes a field theory quantum?\nanswer: the path integral\nTechnically, there are two equivalent answers. One is so elegant and useful that it is overwhelmingly preferred to the other (though this dominance is waning), and it is known as the path integral formalism, gifted to the world by the illustrious Dick Feynman (as his friends called him). For the curious, the alternative is known as the canonical formalism, but we will not delve into it here.\nThe Path Integral Formalism In this formalism, we are given a ground state of our quantum system $\\ket\\Omega$. The essence of Feynman\u0026rsquo;s insight is that the amplitude (a complex number) for any quantum state to evolve into another state given by the path or functional integral over every possible evolution of the system. For the ground state to evolve into the ground state this looks like:\n$$ \\braket{\\Omega \\vert \\Omega} = \\int \\mathcal{D}\\varphi \\ e^{\\frac{i}{\\hbar}S[\\varphi]}. $$\nWe sum up the $e^{\\frac{i}{\\hbar}S[\\varphi]}$ for every possible \u0026ldquo;path\u0026rdquo; $\\varphi(\\vec x, t)$. This is the ingredient than when infused into our field theory gives us a quantum view of the world: our fields don\u0026rsquo;t take just one trajectory, they take every trajectory at the same time. It can be shown that this reduces to the classical case in the limit $\\hbar \\rightarrow 0$, and we will now drop $\\hbar$ for brevity.\nThe correlators are also amplitudes and look like:\n$$ \\langle \\varphi_1 \\dots \\varphi_n \\rangle := \\langle \\Omega \\vert T\\{\\hat\\varphi(x_1) \\dots \\hat\\varphi(x_n)\\} \\vert \\Omega \\rangle = \\int \\mathcal{D}\\varphi \\ e^{iS[\\varphi]} \\varphi(x_1) \\cdots \\varphi(x_n), $$\nwhich is the \u0026ldquo;probability\u0026rdquo; of observing field excitations (particles), created by field operators $\\hat\\varphi(\\vec x_i, t_i)$ ordered by time, so that earlier time operators act before later time operators. This business of earlier \u0026amp; later times is what the time ordering operator $T\\{\\dots\\}$ has to do with, though I am not going to go into the details of how it works, as it is not essential to the story I am trying to tell. In fact, here it just works, so we don\u0026rsquo;t even have to worry about it.\nThe Generating Functional We can now build the central object of quantum field theory: the generating functional\n$$ Z[J] = \\int \\mathcal{D}\\varphi \\ e^{iS[\\varphi] + i \\int d^4x J(x) \\varphi(x)}. $$\nHere, $J(x)$ is \u0026ldquo;the classical source\u0026rdquo;, and via the functional derivative, allows us to define $n$-point amplitudes in terms of $Z(J)$:\n$$ \\langle \\varphi_1 \\dots \\varphi_n \\rangle = (-i)^n \\frac{1}{Z(0)} \\frac{\\delta^n Z[J]}{\\delta J(x_1)\\cdots \\delta J(x_n)} \\Bigg\\vert_{J = 0} $$\nWhich can be easily derived using the identity\n$$ \\frac{\\delta J(x)}{\\delta J(x_i)} = \\delta(x - x_i). $$\nNow let\u0026rsquo;s perform a wick rotation, brushing the details and justification under the rug like any good physicist would, as well as expand the Lagrangian as the sum of a term quadratic in $\\varphi$ and a potential functional $V[\\varphi]$ containing everything else. Also, let\u0026rsquo;s introduce the notation $J \\cdot \\varphi = \\int d^4x J(x) \\varphi(x)$, for a vector $J(x)$, where the usual matrix multiplication rules are implied. The result is:\n$$ Z[J] = \\int \\mathcal{D}\\varphi \\ e^{-\\frac{1}{2} \\varphi \\cdot A \\cdot \\varphi - V[\\varphi] + J \\cdot \\varphi}. $$\nThe Central Identity As physicists lets abuse our notation even further, expanding $V[\\varphi]$ as power series in $\\varphi$:\n$$ V[\\varphi] = \\sum_{k \\geq 3} \\frac{1}{k!}\\lambda_k \\varphi^k $$\nHere what we mean is that each term $\\varphi^k$ is some combination of $k$ field elements and their derivatives (which are all included in our big vector $\\varphi$). The $\\lambda_k$s are the coupling constants of these interaction terms.\nThis allows us to write, using some slick notation,\n$$ \\begin{align*} Z[J] \u0026amp;= \\int \\mathcal{D}\\varphi \\ e^{-V[\\varphi]} e^{-\\frac{1}{2} \\varphi \\cdot A \\cdot \\varphi + J \\cdot \\varphi} \\\\\\ \u0026amp;= e^{-V\\left[ \\frac{\\delta}{\\delta J}\\right]} \\int \\mathcal{D}\\varphi e^{-\\frac{1}{2} \\varphi \\cdot A \\cdot \\varphi + J \\cdot \\varphi}. \\\\\\ \\end{align*} $$\nWhich, in combination with an generalized identity for multidimensional Gaussian integrals (derived in the next section), gives us, up to normalization, the central identity of quantum field theory:\n$$ \\boxed{Z[J] = e^{-V\\left[ \\frac{\\delta}{\\delta J}\\right]} e^{\\frac{1}{2} J \\cdot A^{-1} \\cdot J}} $$\nThis formula is a very useful for calculating correlators by taking derivatives and doing perturbation theory. There is a caveat - we can\u0026rsquo;t guarantee that $A$ is invertible. Resolving this issue, when it arises, like in the case of the Maxwell action, takes some extra work, specifically a topic called gauge fixing, which will be discussed in a follow-up post.\nGaussian Integrals Assuming familiarity with the famous one dimensional Gaussian integral, it is relatively straight forward to generalize it to the following form:\n$$ I = \\int_{-\\infty}^\\infty \\ dx e^{-\\frac{1}{2}ax^2 + Jx} = \\left( \\frac{2\\pi}{a}\\right)^{\\frac{1}{2}}e^{J^2 / 2a}. $$\nWhich is a result of \u0026ldquo;completing the square\u0026rdquo;:\n$$ -\\frac{1}{2}ax^2 + Jx = -\\frac{a}{2}\\left(x^2 - \\frac{2Jx}{a}\\right) = -\\frac{a}{2}\\left(x - \\frac{J}{a}\\right)^2 + \\frac{J^2}{2a}. $$\nChanging variables by taking $x \\rightarrow x - J/a$ does not change the measure $dx$ and allows the integral to be done as usual, giving the result above.\nNow let\u0026rsquo;s consider the integral\n$$ I\u0026rsquo; = \\int_{-\\infty}^\\infty \\prod_{j=1}^N dx_j \\ e^{-\\frac{1}{2} x \\cdot A \\cdot x + J \\cdot x}, $$\nwith $A$ being a real symmetric $N\\times N$ matrix, and $x, J$ as $N$-dimensional vectors.\nThe trick to this multidimensional version of our original integral, $I\u0026rsquo;$ is to convert it into a product of the original integral $I$. We can do this by diagonalizing $A = O^{-1} D O$, with an orthogonal transformation $O$, and a diagonal matrix $D$. We can then let $y_i = \\sum_j O_{ij} x_j$; the measure is invariant under this relabeling and the multidimensional integral breaks up into a product of \u0026ldquo;normal\u0026rdquo; integrals like so:\n$$ \\begin{align*} I\u0026rsquo; \u0026amp;= \\int_{-\\infty}^\\infty \\prod_{j=1}^N dx_j \\ e^{-\\frac{1}{2} x \\cdot O^{-1} D O \\cdot x + J \\cdot x } \\\\\\ \u0026amp;= \\prod_{j=1}^N\\int_{-\\infty}^\\infty dy_j \\ \\exp \\left\\{-\\frac{1}{2} D_{jj} \\ y_j^2 + (OJ)_j \\ y_j \\right\\} \\\\\\ \u0026amp;= \\prod_{j=1}^N \\ \\left( \\frac{2\\pi}{D_{jj}}\\right)^{\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\left(\\sum_k O_{jk} J_k \\right)^2 \\ D^{-1}_{jj} \\right\\} \\\\\\ \u0026amp;= \\left( \\frac{(2\\pi)^N}{\\prod_j D_{jj}}\\right)^{\\frac{1}{2}} \\exp \\left\\{-\\frac{1}{2}\\sum_{j,k,l} J_l O_{jl} D^{-1}_{jj} O_{jk} J_k \\right\\} \\\\\\ \u0026amp;= \\left( \\frac{(2\\pi)^N}{\\det A}\\right)^{\\frac{1}{2}} e^{-\\frac{1}{2} J \\cdot O^{-1} D^{-1} O \\cdot J } \\\\\\ \u0026amp;= \\left( \\frac{(2\\pi)^N}{\\det A}\\right)^{\\frac{1}{2}} e^{-\\frac{1}{2} J \\cdot A^{-1} \\cdot J} \\\\\\ \\end{align*} $$\nThis is what we wanted, phew! Now, if you made it this far you are either regretting it or wondering how this applies to the continuous fields and operators in our original path integral - the answer is that the definition of the path integral measure looks like\n$$ \\int \\mathcal{D}\\varphi = \\lim_{N \\rightarrow \\infty} \\mathcal{N} \\int_{-\\infty}^\\infty \\prod_{j=1}^N d\\varphi_j. $$\nHere $\\mathcal{N}$ is a complicated normalization constant and $\\varphi_j = \\varphi(x_j)$ discretises the field so that in the limit $N \\rightarrow \\infty$ we go towards the continuum. Going back and forth from the lattice (discretised space) to the continuum is an essential skill for field theorists. A big open problem in physics is quantum gravity; one approach (see my earlier post on lattice quantum gravity, seeks to find a way to take the continuum limit of a discretised version of the Einstein-Hilbert action, a task that is not trivial.\n","permalink":"https://aarontrowbridge.github.io/posts/central-identity-of-QFT/","summary":"Quantum field theory is the study of various types of fields, the interactions between these fields, and the correlation functions describing the dynamics of the entire system. Fields are really just functions that label each spacetime coordinate with some type of mathematical object. We can talk about spinor fields, scalar fields, and even gauge fields, which can be described by vectors or tensors that transform in certain ways under an associated gauge group.","title":"the central identity of quantum field theory"},{"content":"The development of mathematical finance, much like the processes it aims to study, has had a particularly jumpy history. A major leap came in 1973, when the Black-Scholes option pricing model was published and mathematically understood. The essential idea is to model the underlying asset $S_t$ of an option as a geometric brownian motion, with a stochastic differential equation (SDE), given by\n$$ dS_t = \\mu S_t \\ dt + \\sigma S_t \\ dW_t, $$\nwhich contains a drift parameter $\\mu$ and a constant volatility parameter $\\sigma$. Taking the price of an option at time $t$ to be a function $C(S,t)$, Ito\u0026rsquo;s lemma applies and the famous Black-Scholes formula can be derived.\nThe first solved stochastic volatility model appeared in 1993, due to Steven Heston1, which aimed to correct the shortcomings of the Black-Scholes model and is what will be studied here.\nThe Heston Model This model is essentially the same as the Black-Scholes model, except that on top of modelling the underlying asset $S$ as a stochastic process, the volatility parameter is also modelled as a stochastic process. In terms of SDEs\n$$ \\begin{align*} dS_t \u0026amp;= \\mu S_t \\ dt + \\sqrt{v_t} S_t \\ dW_t^{(1)}, \\\\\\ dv_t \u0026amp;= \\kappa(\\bar{v} - v_t) \\ dt + \\sigma \\sqrt{v_t} \\ dW_t^{(2)}, \\\\\\ \\end{align*} $$\nwith\n$$ dW_t^{(1)}dW_t^{(2)} = \\rho dt. $$\nThe five parameters of the model:\n$v_0$, the initial volatility $\\bar{v}$, the long variance $\\rho$, the correlation of the two Wiener processes $\\kappa$, the rate at which $v_t$ reverts to $\\bar{v}$ $\\sigma$, the volatility of the volatility We will use $\\mathbf{\\theta} = [v_0, \\bar{v}, \\rho, \\kappa, \\sigma]^\\top$ to represent the vector of parameters of the model.\nPricing Derivatives For a vanilla call option with strike $K$ and maturity $T$, where the underlying asset has a spot price $S_0$ and interest rate $r$, we can use our model to estimate the price:\n$$ \\begin{align*} C(\\mathbf{\\theta}; K, T) \u0026amp;= e^{-rT} \\mathbb{E} \\left[\\left( S_T - K \\right)1_{{S_T \\geq K } }(S_T) \\right] \\\\\\ \u0026amp;= e^{-rT} \\left( \\mathbb{E} \\left[ S_T \\ 1_{{S_T \\geq K }}(S_T)\\right] - K \\mathbb{E} \\left[ 1_{{S_T \\geq K }}(S_T)\\right] \\right) \\\\\\ \u0026amp;= S_0 P_1 - e^{-rT}K P_2 \\\\\\ \\end{align*} $$\nHere $P_1$ and $P_2$ are given as\n$$ \\begin{align*} P_1 \u0026amp;= \\frac{1}{2} + \\frac{1}{\\pi}\\int_0^{\\infty} \\operatorname{Re} \\left( \\frac{e^{-iu \\log K}}{iuF} \\varphi(\\theta; u - i, T) \\right) \\mathrm{d}u, \\\\\\ P_2 \u0026amp;= \\frac{1}{2} + \\frac{1}{\\pi}\\int_0^{\\infty} \\operatorname{Re} \\left( \\frac{e^{-iu \\log K}}{iu} \\varphi(\\theta; u, T) \\right) \\mathrm{d}u, \\\\\\ \\end{align*} $$\nwith $F = S_0 e^{rT}$ and $\\varphi(\\theta; u, t)$ is the characteristic function of the logarithm of the stock price model.\nThe characteristic function was given in its original form by Heston and subsequently in other forms that tried to avoid discontinuities and be analytically differentiable. The representation that achieved both of these goals was given by Cui et al.2 in 2016. It is given by\n$$ \\varphi(\\theta; u, t) = \\exp \\left\\{ iu \\left( \\log S_0 + rt - \\frac{t \\kappa \\bar{v} \\rho}{\\sigma} \\right) - v_0 A + \\frac{2 \\kappa \\bar{v}}{\\sigma^2} D\\right\\} $$\nwhere\n$$ \\begin{align} A \u0026amp;:= \\frac{A_1}{A_2}, \\\\\\ A_1 \u0026amp;:= (u^2 + iu)\\sinh \\frac{dt}{2}, \\\\\\ A_2 \u0026amp;:= d \\cosh \\frac{dt}{2} + \\xi \\sinh \\frac{dt}{2}, \\\\\\ \\end{align} $$\nand\n$$ D := \\log d + \\frac{(\\kappa - d)t}{2} - \\log \\left( \\frac{d + \\xi}{2} + \\frac{d - \\xi}{2}e^{-dt}\\right), $$\nwith\n$$ \\begin{align} d \u0026amp;:= \\sqrt{\\xi^2 + \\sigma^2(u^2 + iu)}, \\\\\\ \\xi \u0026amp;:= \\kappa - i \\sigma \\rho u, \\\\\\ \\end{align} $$\nCalibrating The Model In order to calibrate the parameters of our model to the observed market data we will seek to minimize the difference between the market price of a vanilla call option $C^*(K_i, T_i)$ and our model\u0026rsquo;s prediction $C(\\theta; K_i, T_i)$ for various strikes and maturities, $K_i$ and $T_i$ respectively. We compute a residual vector $\\mathbf{r}(\\theta) \\in \\mathbb{R}^n$ for the $n$ options, with $i$th component\n$$ r_i(\\theta) := C(\\theta; K_i, T_i) - C^*(K_i, T_i), \\qquad i = 1,\\dots,n. $$\nThe optimization problem can then be stated as nonlinear least squares problem\n$$ \\min_{\\theta \\in \\mathbb{R}^m}f(\\theta). $$\nWhere $m = 5$, in this case, is the dimesion of the parameter vector and\n$$ f(\\theta) := \\frac{1}{2} |\\mathbf{r}(\\theta)|^2 = \\frac{1}{2} \\mathbf{r}^\\top(\\theta)\\mathbf{r}(\\theta). $$\nHere we have $n \\gg m$ so the optimization problem is overdetermined but there is significant computational cost involved in evaluating $C(\\theta; K, T)$. Additionally an analytic expression for the gradient was not known until just recently, also given by Cui et al.\nAnalytic Gradient We use $\\nabla = \\partial / \\partial \\theta$ as the gradient operator with respect to the parameter vector $\\theta$ and $\\nabla \\nabla^\\top$ for the Hessian operator. We are interested in calculating the gradient and Hessian of the objective function $f(\\theta)$:\n$$ \\begin{align} \\nabla f \u0026amp;= \\nabla \\left( \\frac{1}{2} \\mathbf{r}^\\top \\mathbf{r} \\right) \\\\\\ \u0026amp;= \\nabla \\mathbf{r}^\\top \\mathbf{r} \\\\\\ \u0026amp;= \\mathbf{J} \\mathbf{r} \\\\\\ \\end{align} $$\nwhere $\\mathbf{J} := \\nabla \\mathbf{r}^\\top$ is the Jacobian matrix of the residual vector $\\mathbf{r}$ which has elements\n$$ J_{ij} = \\frac{\\partial r_j}{\\partial \\theta_i}. $$\nThe Hessian of the objective function is then\n$$ \\begin{align*} \\nabla \\nabla^\\top f \u0026amp;= \\nabla \\nabla^\\top \\left( \\frac{1}{2} \\mathbf{r}^\\top \\mathbf{r} \\right) \\\\\\ \u0026amp;= \\nabla \\left(\\nabla \\mathbf{r}^\\top \\mathbf{r} \\right)^\\top \\\\\\ \u0026amp;= \\nabla \\left( \\mathbf{r}^\\top \\left( \\nabla \\mathbf{r}^\\top \\right)^\\top \\right) \\\\\\ \u0026amp;= \\nabla \\mathbf{r}^\\top \\left( \\nabla \\mathbf{r}^\\top \\right)^\\top + \\sum_i r_i \\nabla \\nabla^\\top r_i \\\\\\ \u0026amp;= \\mathbf{J}\\mathbf{J}^\\top + \\sum_i r_i \\mathbf{H}(r_i) \\\\\\ \\end{align*} $$\nwhere $\\mathbf{H}(r_i) := \\nabla \\nabla^\\top r_i$ is the Hessian matrix for each residual $r_i$ with components\n$$ H_{jk}(r_i) = \\frac{\\partial^2r_i}{\\partial \\theta_j \\partial \\theta_k}. $$\nFor details of the calculation of $\\nabla C(\\theta; K, T)$ needed in the implementation of the optimization algorithm described below see Theorem 1 in the paper by Cui et al. As can be inferred from the form of the characteristic function it is not clean!\nThe Levenberg-Marquardt Method The LM algorithm aims to solve the problem of minimizing the squared residuals of the model. We wish to find\n$$ \\hat\\theta \\in \\arg\\min_{\\theta} f(\\theta). $$\nHere, $\\hat\\theta$ is not necessarily a global minimum (though we hope to show it is), which is why the member notation is used in place of equality.\nThe derivation of the parameter space search step goes as follows:\n$$ \\begin{align*} f(\\theta + \\Delta \\theta) \u0026amp;\\approx f(\\theta) + \\Delta \\theta^\\top \\nabla f + \\frac{1}{2} \\Delta \\theta^\\top \\left(\\nabla \\nabla^\\top f \\right) \\Delta \\theta. \\\\\\ \u0026amp;= f(\\theta) + \\Delta \\theta^\\top \\nabla f + \\frac{1}{2} \\Delta \\theta^\\top \\left(\\mathbf{J} \\mathbf{J}^\\top + \\sum_i r_i \\mathbf{H}(r_i) \\right)\\Delta \\theta. \\\\\\ \u0026amp;\\approx f(\\theta) + \\Delta \\theta^\\top \\nabla f + \\frac{1}{2} \\Delta \\theta^\\top \\mathbf{J} \\mathbf{J}^\\top \\Delta \\theta. \\\\\\ % f(\\theta + \\Delta \\theta) \u0026amp;= \\frac{1}{2}\\mathbf{r}(\\theta + \\Delta \\theta)^\\top \\mathbf{r}(\\theta + \\Delta \\theta) \\\\\\ % \u0026amp;\\approx \\frac{1}{2} \\left( \\mathbf{r}(\\theta) + \\mathbf{J}^\\top \\Delta \\theta \\right)^\\top\\left( \\mathbf{r}(\\theta) + \\mathbf{J}^\\top \\Delta \\theta \\right) \\\\\\ % \u0026amp;= \\frac{1}{2} \\left( \\mathbf{r}(\\theta)^\\top \\mathbf{r}(\\theta) + 2 \\mathbf{r}(\\theta)^\\top \\mathbf{J}^\\top \\Delta \\theta + \\Delta \\theta^\\top \\mathbf{J} \\mathbf{J}^\\top \\Delta \\theta \\right) \\\\\\ % \u0026amp;= f(\\theta) + \\nabla f^\\top \\Delta \\theta + \\frac{1}{2} \\Delta \\theta^\\top \\mathbf{J} \\mathbf{J}^\\top \\Delta \\theta. \\\\\\ \\end{align*} $$\nWhere the last line is a valid approximation when the residuals $r_i$ or the Hessian of the residuals $\\mathbf{H}(r_i)$ are small.\nNow taking the derivative with respect to $\\Delta \\theta$, setting it equal to zero, and flipping the sign (to descend the space), we arrive at the search step:\n$$ \\Delta \\theta = \\left( \\mathbf{J}\\mathbf{J}^\\top + \\mu \\mathbf{I} \\right)^{-1} \\nabla f. $$\nHere the $\\mu \\mathbf{I}$ term is Levenberg\u0026rsquo;s contribution, and acts as a damping factor that is dynamically adjusted to interpolate between the method of steepest descent and the Gauss-Newton method.\nWhen the residuals are large (the model is far from the optimum), $\\mu$ is set large, resulting in a steepest descent approach. When close to the optimum, $\\mu$ is made small, resulting in the Gauss-Newton method.\nFor details of the LM calibration algorithm see Algorithm 4.1 in Cui et al.2 Implementation and Results A incomplete repository of code can be found at my github in LevyProcesses. I will be following up with a new post when I have a working model.\nReferences Heston (1993) \u0026ldquo;Closed-Form Solution for Options with Stochastic Volatility with Applications to Bond and Currency Options\u0026rdquo; pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCui et al. (2015) \u0026ldquo;Full and fast calibration of the Heston stochastic volatility model\u0026rdquo; arxiv:1511.08718\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://aarontrowbridge.github.io/posts/the-heston-model/","summary":"The development of mathematical finance, much like the processes it aims to study, has had a particularly jumpy history. A major leap came in 1973, when the Black-Scholes option pricing model was published and mathematically understood. The essential idea is to model the underlying asset $S_t$ of an option as a geometric brownian motion, with a stochastic differential equation (SDE), given by\n$$ dS_t = \\mu S_t \\ dt + \\sigma S_t \\ dW_t, $$","title":"the Heston stochastic volatility model"},{"content":"Pseudovectors, or axial vectors as they are sometimes referred to as, are commonly encountered, but mysterious mathematical objects. In physics they arise in many different areas, particularly when cross products are involved - e.g., magnetic fields, angular momentum, and torque.\nThe goal of this post is to address two seemingly different definitions of a pseudovector, in 3 dimensions, and in the process unravel this mystery.\nDefinition 1 A pseudovector is a tensor on $\\mathbb{R}^3$ that transforms like a vector under proper rotations, but picks up a sign under an improper rotation, like a reflection.\nDefinition 2 A pseudovector is a tensor on $\\mathbb{R}^3$ that transforms like a vector under proper rotations, and is invariant under inversion, a tranformation also known as parity.\nBackground Before jumping into some calculations I will briefly go over some of the aforementioned terminology.\nTensors I could opt to take the easy way out and use the physicist\u0026rsquo;s definition of a tensor: a tensor is a quantity that transforms like a tensor, and leave it at that.\nBut, I can\u0026rsquo;t help introducing some notation, and define $T$ to be a (real) tensor of type $(r, s)$, on an $n$-dimensional vector space $V$ over $\\mathbb{R}$ to be the multilinear map\n$$ T : \\underbrace{V \\times \\cdots \\times V}_{r \\text{ times}} \\ \\times \\underbrace{V^* \\times \\cdots \\times V^*}_{s \\text{ times}} \\rightarrow \\mathbb{R} $$\nwhere $V^*$ is the dual vector space to $V$, i.e., the space of linear maps from $V \\rightarrow \\mathbb{R}$. We will refer to the space of all such tensors to be $\\mathcal{T}^r_s(V) = \\mathcal{T}^r_s$, when the underlying vector space is understood. It should also be noted that all of these definitions apply to complex vector spaces.\nGiven a basis ${e_i}_{i=1\\dots n}$ for $V$, and corresponding dual basis ${e^i}$ for $V^*$, satisfying $e^j(e_i) = \\delta^j_i$, we can look at the components of a tensor $T \\in \\mathcal{T}^r_s$,\n$$ T_{i_1 \\dots i_r}^{\\ \\ \\ \\ \\ \\ \\ \\ \\ j_1 \\dots j_s} = T(e_{i_1}, \\dots, e_{i_r}, e^{j_1}, \\dots, e^{j_s}) $$\nWhile the convention here of putting indices on indices might seem confusing, it is quite elegant once gotten used to. For arbitrary rank tensors we quickly run out of conventional indices, $i, j, k, l, \\dots$, and introducing subscripts on indices is very nice. Of course, for most tensors we only need a few indicesm, so we will use more conventional notation, for example a tensor $T \\in \\mathcal{T}^1_1$ has components we can label as\n$$ T_i^{\\ j} = T(e_i, e^j) $$\nwhich rightfully resembles the matrix elements of a linear operator. There are some subtle differences that going into will, unfortunately, take us to far astray. I will postpone a detailed look at matrix representations of linear operators to another blog post.\nTransformations In what follows, the Einstein summation convention, where like indices are implicitly summed over, will be used.\nIn some sense the most important aspect of vectors, tensors, pseudovectors, or any other such object you can come up with, is how it changes when we change basis. This is the essence of the physicist\u0026rsquo;s definition. For example, we know our basis vectors transform covariantly:\n$$ e_i \\rightarrow e_{i\u0026rsquo;} = A^j_{i\u0026rsquo;}e_j $$\nand the dual basis vectors transform contravariantly:\n$$ e^i \\rightarrow e^{i\u0026rsquo;} = A_j^{i\u0026rsquo;}e^j $$\nThen, our $(1, 1)$-tensor $T$\u0026rsquo;s components transform like\n$$ \\begin{align*} T_i^j \\rightarrow T_{i\u0026rsquo;}^{j\u0026rsquo;} \u0026amp;= T(e_{i\u0026rsquo;}, e^{j\u0026rsquo;}) \\\\\\ \u0026amp;= T(A^k_{i\u0026rsquo;}e_k, A_l^{j\u0026rsquo;}e^l) \\\\\\ \u0026amp;= A^k_{i\u0026rsquo;} A_l^{j\u0026rsquo;} T(e_k, e^l) \\\\\\ \u0026amp;= A^k_{i\u0026rsquo;} A_l^{j\u0026rsquo;} T_k^l \\\\\\ \\end{align*} $$\nThis procedure generalizes to tensors of arbitrary order, which is what it means for a quantity to transform like a tensor!\nNow, for a vector $v \\in V$, we can represent the vector in a basis as $v = v^j e_j$, a linear combination of the basis vectors, $e_i$, and the components, $v^i$, of the vector in that basis. Then under a basis transformation the components of our vector will transform, but the vector itself has to remain the same because it is independent of our basis choice, i.e., $v = v'$\n$$ \\begin{align*} v \\rightarrow v\u0026rsquo; \u0026amp;= v^{i\u0026rsquo;}e_{i\u0026rsquo;} \\\\\\ \u0026amp;= v^{i\u0026rsquo;}A_{i\u0026rsquo;}^j e_j \\\\\\ \u0026amp;\\Rightarrow v^j = v^{i\u0026rsquo;}A_{i\u0026rsquo;}^j \\\\\\ \\end{align*} $$\nand after commuting terms (which is perfectly fine in index notation) and relabeling indices, gives us the familiar transformation rules for vector components:\n$$ v^{i\u0026rsquo;} = A^{i\u0026rsquo;}_j v^j $$\nWhich is what it means for a quantity to transform as a vector.\nRotations and Reflections An immediate consequence of the vector transformation rules is that\n$$ \\begin{align*} v^j \u0026amp;= A_{i\u0026rsquo;}^j v^{i\u0026rsquo;} \\\\\\ \u0026amp;= A_{i\u0026rsquo;}^j \\left(A^{i\u0026rsquo;}_k v^k \\right) \\\\\\ \u0026amp;= A^{i\u0026rsquo;}_k A_{i\u0026rsquo;}^j v^k \\\\\\ \u0026amp;\\Rightarrow A^{i\u0026rsquo;}_k A_{i\u0026rsquo;}^j = \\delta^j_k \\\\\\ \\end{align*} $$\nwhich, in matrix notation, reads $A A^T = I \\Rightarrow A^{-1} = A^T$.\nThis condition defines the group of rotations and reflections in 3 dimensions: the orthogonal group $O(3)$. Where for $R \\in O(3)$ we can show that $\\det R = \\pm 1$ like so:\n$$ \\begin{align*} 1 \u0026amp;= (\\det R) (\\det R)^{-1} \\\\\\ \u0026amp;= (\\det R) (\\det R^{-1}) \\\\\\ \u0026amp;= (\\det R) (\\det R^T) \\\\\\ \u0026amp;= (\\det R)^2 \\\\\\ \u0026amp;\\Rightarrow \\det R = \\pm 1 \\\\\\ \\end{align*} $$\nWhere I used the fact that $\\det A^T = \\det A$ without proof.\nNow thinking about these two cases, we can recognize the set of $3\\times 3$ matrices $\\{ R \\in O(3) \\ \\vert \\ \\det R = 1 \\}$ as the group of proper rotations, which is also called the special orthogonal group, $SO(3)$. That leaves us with just the set of improper rotations, whose members I will refer to as $\\hat R$ and can be written as $\\hat R = (-I)R$, where $R \\in SO(3)$.\nThe improper rotation $-I$ is what was previously referred to as inversion, or parity. All other improper rotations can be thought of as rotations followed by reflections in the plane perpendicular to the axis of rotation.\nPseudovectors We can finally get to the topic at hand! The extra work we put in will now aid us as we look at pseudovectors from couple different angles and hopefully mitigate some of the confusion, which I realize might have only built up since we began.\nCross Products and the Levi-Civita Tensor The cross product is a very special operation in physics and mathematics, as it arises all over the place in various physical theories and is mathematically unique to 3-dimensions, and is in fact a pseudovector as we will see. Traditionally, given 2 vectors, $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^3$, we can can take their cross product\n$$ \\mathbf{a} \\times \\mathbf{b} = | \\mathbf{a} | | \\mathbf{b} | \\sin \\theta \\ \\hat{\\mathbf{n}} = a b \\sin \\theta \\ \\hat{\\mathbf{n}}, $$\nwhere $\\theta$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$, and $\\hat{\\mathbf{n}}$ is the unit vector normal to the surface spanned by $\\mathbf{a}$ and $\\mathbf{b}$ and oriented by the right hand rule.\nWe can also use index notation to write the components of a vector, $v = a \\times b$, as\n$$ v^i = \\underbrace{\\sum_{j, k} \\epsilon^i_{\\ \\ jk}a^j b ^k}_{\\text{summing explicitly}} = \\epsilon^i_{\\ \\ jk}a^j b^k $$\nHere we have introduced the Levi-Civita symbol, $\\epsilon_{ijk}$, which is totally antisymmetric and is related to the Levi-Civita tensor on $\\mathbb{R}^3$, defined by\n$$ \\epsilon(u, v, w) \\equiv (u \\times v) \\cdot w, \\quad u, v, w \\in \\mathbb{R}^3. $$\nThis definition may appear familiar - it is the formula for the oriented volume of a parallelepiped, formed by 3 vectors in $\\mathbb{R}^3$. With this definition in hand, as well as an orthonormal basis ${ e_i }$ we can now define the Levi-Civita symbol,\n$$ \\epsilon_{ijk} \\equiv \\epsilon(e_i, e_j, e_k) = \\left\\{ \\begin{array}{cl} +1 \u0026amp; \\text{if } (i, j, k) \\text{ is a cyclic permutation of } (1, 2, 3)\\\\\\ -1 \u0026amp; \\text{if } (i, j, k) \\text{ is an anticyclic permutation of } (1, 2, 3) \\\\\\ 0 \u0026amp; \\text{otherwise} \\end{array} \\right. $$\nIndex Conventions Up until now, we have been rigorously keeping track of upstairs and downstairs indices, which has encoded information about wether a quantity transforms covariantly or contravariantly. Using this convention, we can raise and lower indices using the euclidean metric, $\\delta_{ij} = \\delta_i^j = \\delta^{ij}$, e.g., $T^{ij} = \\delta^{jk} T^i_{\\ \\ k}$ or $\\ T^i_{\\ \\ j} = \\delta_{jk} T^{ik}$. Because we are living in euclidean space and have the euclidean metric we can relax our index convention, and freely raise and lower indices of objects like $\\epsilon_{ijk}$ when convenient. This is justified since, for instance,\n$$ \\epsilon_{ijk} = \\delta_{il} \\epsilon^l_{\\ \\ jk} = \\delta^i_l \\epsilon^l_{\\ \\ jk} = \\epsilon^i_{\\ \\ jk}. $$\nSo, even though I haven\u0026rsquo;t yet explicitly forbidden it, we will now implicitly sum over any like indices, not just those that appear pairwise upstairs and downstairs.\nDeterminants and Pseudovector Transformations With the knowledge we have built up, and a little mathematical sorcery, which I will elucidate in a later post, we can now define the determinant of an $n \\times n$ matrix $A$:\n$$ \\vert A \\vert \\equiv \\det A = \\epsilon(A_1, \\dots, A_n) = \\sum_{i_1,\\dots,i_n} \\epsilon_{i_1,\\dots,i_n} A^{i_1}_1 \\dots A^{i_n}_n $$\nwhere $A_i$ is the $i$-th column of $A$. Which implies, with a little bit of extra work, and particularly in the 3-dimensional case, that\n$$ \\epsilon_{ijk} A^{i}_l A^{j}_m A^{k}_n = \\vert A \\vert \\epsilon_{lmn} $$\nLet\u0026rsquo;s now see how the components of our pseudovector, $v^i = \\epsilon^i_{\\ \\ j k}a^j b^k$, transform, when we take $e_i \\rightarrow e_{i\u0026rsquo;} = A^j_{i\u0026rsquo;}e_j$. Plugging in, we see that\n$$ \\begin{align*} v^i \\rightarrow v^{i\u0026rsquo;} \u0026amp;= \\epsilon^{i\u0026rsquo;}_{\\ \\ j\u0026rsquo;k\u0026rsquo;}A^{j\u0026rsquo;}_l a^l A^{k\u0026rsquo;}_m b^m \\\\\\ \u0026amp;= \\delta^{i\u0026rsquo;p\u0026rsquo;} \\epsilon_{p\u0026rsquo;j\u0026rsquo;k\u0026rsquo;} A^{j\u0026rsquo;}_l A^{k\u0026rsquo;}_m a^l b^m \\\\\\ \u0026amp;= \\epsilon_{p\u0026rsquo;j\u0026rsquo;k\u0026rsquo;} A^{i\u0026rsquo;}_{q} A^{p\u0026rsquo;}_{q} A^{j\u0026rsquo;}_l A^{k\u0026rsquo;}_m a^l b^m \\\\\\ \u0026amp;= \\vert A \\vert A^{i\u0026rsquo;}_{q} \\epsilon_{qlm} a^l b^m \\\\\\ \u0026amp;= \\vert A \\vert A^{i\u0026rsquo;}_j v^j \\\\\\ \\end{align*} $$\nThus, we arrive at the resolution to the original dilemma - the two definitions are equivalent:\nUnder Inversion (i.e., $A = -I$),\n$$ v \\rightarrow v\u0026rsquo; = \\det(-I) (-I)v = v, $$\nwhich is exactly what it means for a pseudovector to be invariant under inversion.\nUnder a General Improper Rotation (i.e., $A = (-I)R$),\n$$ v \\rightarrow v\u0026rsquo; = \\det\\big((-I) R\\big) A v = \\det(-I) \\det(R) A v = - A v, $$\nwhich is what it means for a pseudovector to pick up a minus sign.\nNote that in even dimensions, $\\det(-I) = 1$, which is why we needed to restrict ourselves to $\\mathbb{R}^3$.\nIf you made it this far, thank you! Stay tuned for a follow up article on bivectors, applications, and generalizations (e.g., pseudoscalars and pseudotensors).\n","permalink":"https://aarontrowbridge.github.io/posts/pseudovectors/","summary":"Pseudovectors, or axial vectors as they are sometimes referred to as, are commonly encountered, but mysterious mathematical objects. In physics they arise in many different areas, particularly when cross products are involved - e.g., magnetic fields, angular momentum, and torque.\nThe goal of this post is to address two seemingly different definitions of a pseudovector, in 3 dimensions, and in the process unravel this mystery.\nDefinition 1 A pseudovector is a tensor on $\\mathbb{R}^3$ that transforms like a vector under proper rotations, but picks up a sign under an improper rotation, like a reflection.","title":"pseudovectors"},{"content":"Just about a year ago today, I began working on implementing an algorithm my undergrad research advisor had devised to speed up the Metropolis algorithm, in the regime where the acceptance probability is very low, which is the case in lattice simulations of quantum gravity.\nQuantum Gravity Physics has experienced its most rapid advancement when theories are unified:\nelectromagnetism $\\leftarrow$ electricity + magnetism + light general relativity $\\leftarrow$ special relativity + curved space-time (gravity) quantum field theory $\\leftarrow$ quantum mechanics + special relativity + electromagnetism + matter + nuclear forces And hopefully soon\u0026hellip;\nquantum gravity $\\leftarrow$ quantum field theory + general relativity It turns out the problem is pretty simple\u0026hellip; to state. We just need to solve the following path integral:\n$$ Z = \\int \\mathcal{D}g \\exp \\left(\\frac{1}{16\\pi G}\\int d^4 x \\sqrt{-g}(R - 2\\Lambda) \\right) $$\nIgnoring for a moment the subtleties of evaluating a path integral in general, in this case it turns out that the term inside of the exponential, the Einstein-Hilbert action, causes some serious problems. $Z$ is perturbatively non-renormalizable, which is technical jargon that means the theory spits out infinities when we try to calculate simple interactions and those infinities only get worse when we attempt to \u0026ldquo;cancel them off\u0026rdquo;, which is a prescription that miraculously works in other (renormalizable) theories, like quantum electrodynamics.\nHope remains though\u0026hellip; while a perturbative expansion of $Z$ blows up, an idea known as asymptotic safety might save the day. This is what the group I became a part of is working on. More specifically we are using a computational technique called dynamical triangulation to calculate a discretized version of $Z$ on a lattice of 4-dimensional simplices.\nThe Freeman Method The beauty of science is that every solution only leads to more problems\u0026hellip; our new problem is that the algorithm we want to use to evolve our simplex lattice is inherently slow, or more accurately picky: it chooses to do nothing about 9,999 times out of every 10,000 times we ask it do something. Which is annoying!\nThe new algorithm basically goes as follows:\nthink about every possible thing we could do and assign a number (probability) to each possibility imagine every possibility as a book on a shelf, with the width of the book corresponding to the probability of picking it blindly approach the book shelf and choose a book This way, each time we want to do something, we actually do something. Which is great!\nAn implementation of this algorithm, in Julia, on the Ising model can be found on the projects page, or directly on github.\nSlides and YouTube Recording There are some more subtleties and technical points - if interested, details can be found in the slides of a presentation I made about this research, or, if very interested, you can also watch a recording of the presentation on YouTube by clicking (tapping) on the image below:\n","permalink":"https://aarontrowbridge.github.io/posts/the-freeman-method/","summary":"Just about a year ago today, I began working on implementing an algorithm my undergrad research advisor had devised to speed up the Metropolis algorithm, in the regime where the acceptance probability is very low, which is the case in lattice simulations of quantum gravity.\nQuantum Gravity Physics has experienced its most rapid advancement when theories are unified:\nelectromagnetism $\\leftarrow$ electricity + magnetism + light general relativity $\\leftarrow$ special relativity + curved space-time (gravity) quantum field theory $\\leftarrow$ quantum mechanics + special relativity + electromagnetism + matter + nuclear forces And hopefully soon\u0026hellip;","title":"the Freeman method"},{"content":"Introductions are tricky. Where are you supposed to begin? I think I\u0026rsquo;ve had a pretty interesting life so far, of course, I am biased, but I\u0026rsquo;m not going to go deep into any of that, right now.\nAs an undergraduate I had the opportunity to work on many interesting projects, most of which were computational, and can be found on my github with poor to nonexistent documentation.\nI thought about cleaning up my past work and presenting it here, but while that might be interesting to readers (if there are any), it sounds boring to me.\nI need to move on to the next thing that drives my curiosity. There is so much more to learn and I have to do myself a favor and focus on what\u0026rsquo;s ahead.\nRoadmap As mentioned on the about page, I have become very interested in quantum field theory, which is a fascinating subject that combines many seemingly unrelated aspects of physics, mathematics, and computer science. Of course, the same could be said of quantum information science, which recently got bumped from the top spot of my research interests. Even still, QFT and QIS are connected, in some very interesting ways that I hope to explore in the future.\nWhile theoretical physics is my passion, I also need to pay rent and fund my culinary explorations. To that aim, I also plan on spending a substantial amount of time researching and practicing methods in data science, specifically those related to machine learning and statistical analysis. I do actually find these fields tantalizing as well.\nBelow is an outline of topics I hope to explore on this blog:\nphysics quantum field theory gauge theory general relativity quantum information theory statistical physics condensed matter theory string theory mathematics differential geometry smooth manifolds topological field theory stochastic differential equations calculus of variations Lie groups \u0026amp; Lie algebras representation theory computational \u0026amp; data science machine learning neural networks kernel methods reinforcement learning topological data analysis statistical inference numerical optimization quantum algorithms data visualization ","permalink":"https://aarontrowbridge.github.io/posts/me-and-my-goals/","summary":"Introductions are tricky. Where are you supposed to begin? I think I\u0026rsquo;ve had a pretty interesting life so far, of course, I am biased, but I\u0026rsquo;m not going to go deep into any of that, right now.\nAs an undergraduate I had the opportunity to work on many interesting projects, most of which were computational, and can be found on my github with poor to nonexistent documentation.\nI thought about cleaning up my past work and presenting it here, but while that might be interesting to readers (if there are any), it sounds boring to me.","title":"me and my goals"}]